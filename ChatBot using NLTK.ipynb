{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ed793f",
   "metadata": {},
   "source": [
    "# ChatBot Using Natural Language ToolKit (NLTK)\n",
    "\n",
    "Natural language processing (NLP) helps computers communicate with humans in their own language and scales other language-related tasks. For example, NLP makes it possible for computers to read text, interpret it, measure sentiment and determine which parts are important. Understanding this will enable you to build the core component of any conversational chatbot. This is the core engine of a conversational chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5bfc124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\John\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installing NLTK Components\n",
    "import nltk\n",
    "\n",
    "# nltk.download_gui()\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c1f8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing All the Required Modules\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e166b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dba61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows and columns of a dataframe instead of a truncated version\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170614e6",
   "metadata": {},
   "source": [
    "## Preprocessing Stage\n",
    "- Convert to lowercase\n",
    "- Tokenize\n",
    "- Remove StopWords\n",
    "\n",
    "Preprocessing stage involves converting text to lowercase, tokenizing it (breaking it into individual words or tokens), and removing stopwords. Tokenization is the process of splitting text into smaller units such as words, phrases, symbols, or other meaningful elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bdf26eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"The old oak tree stood majestically in the center of the lush green meadow, its branches reaching out like welcoming arms.\"\n",
    "sentence2 = \"This is particularly important in today's world where we are swamped with unstructured natural language data on the variety of social media platforms people engage in now-a-days (note -  now-a-days in the decade of 2010-2020)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "077e1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d714d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['particularly', 'important', 'today', 'world', 'swamped', 'unstructured', 'natural', 'language', 'data', 'variety', 'social', 'media', 'platforms', 'people', 'engage', 'days', 'note', 'days', 'decade', '2010', '2020']\n"
     ]
    }
   ],
   "source": [
    "preprocessedSentence = preprocess(sentence2)\n",
    "print(preprocessedSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e22e19",
   "metadata": {},
   "source": [
    "## Lematizing or Extracting Features!\n",
    "- Tagging (POS Tagging) : *Assignement of POS tags to each word.*\n",
    "- Stemming : *Process of reducing words to their root or base form by removing suffixes or prefixes.*\n",
    "- Lemmatize words : *Process of reducing words to their base or dictionary form, known as the lemma. This process involves removing inflections and variations to bring words to a common base form, making them easier to analyze and compare across different contexts*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce61205",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "Tagging involves assigning part-of-speech tags to each word in a given text, indicating its grammatical category and usage in the sentence.\n",
    "\n",
    "Here is the POS tag list:\n",
    "\n",
    "- CC: Coordinating conjunction\n",
    "- CD: Cardinal digit\n",
    "- DT: Determiner\n",
    "- EX: Existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "- FW: Foreign word\n",
    "- IN: Preposition/subordinating conjunction\n",
    "- JJ: Adjective 'big'\n",
    "- JJR: Adjective, comparative 'bigger'\n",
    "- JJS: Adjective, superlative 'biggest'\n",
    "- LS: List marker 1\n",
    "- MD: Modal could, will\n",
    "- NN: Noun, singular 'desk'\n",
    "- NNS: Noun plural 'desks'\n",
    "- NNP: Proper noun, singular 'Harrison'\n",
    "- NNPS: Proper noun, plural 'Americans'\n",
    "- PDT: Predeterminer 'all the kids'\n",
    "- POS: Possessive ending parent's\n",
    "- PRP: Personal pronoun I, he, she\n",
    "- PRP$: Possessive pronoun my, his, hers\n",
    "- RB: Adverb very, silently,\n",
    "- RBR: Adverb, comparative better\n",
    "- RBS: Adverb, superlative best\n",
    "- RP: Particle give up\n",
    "- TO: To go 'to' the store.\n",
    "- UH: Interjection errrrrrrrm\n",
    "- VB: Verb, base form take\n",
    "- VBD: Verb, past tense took\n",
    "- VBG: Verb, gerund/present participle taking\n",
    "- VBN: Verb, past participle taken\n",
    "- VBP: Verb, sing. present, non-3d take\n",
    "- VBZ: Verb, 3rd person sing. present takes\n",
    "- WDT: Wh-determiner which\n",
    "- WP: Wh-pronoun who, what\n",
    "- WP$: Possessive wh-pronoun whose\n",
    "- WRB: Wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "18581166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('particularly', 'RB'), ('important', 'JJ'), ('today', 'NN'), ('world', 'NN'), ('swamped', 'VBD'), ('unstructured', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('variety', 'NN'), ('social', 'JJ'), ('media', 'NNS'), ('platforms', 'NNS'), ('people', 'NNS'), ('engage', 'VBP'), ('days', 'NNS'), ('note', 'VBP'), ('days', 'NNS'), ('decade', 'NN'), ('2010', 'CD'), ('2020', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(preprocessedSentence)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "af14159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['particularly', 'important', 'today', 'world', 'unstructured', 'natural', 'language', 'data', 'variety', 'social', 'media', 'platforms', 'people', 'engage', 'days', 'note', 'days', 'decade']\n"
     ]
    }
   ],
   "source": [
    "# Extracting just the required one's like verbs, nouns, etc\n",
    "def extractTags(tags):\n",
    "    features = []\n",
    "    for tagged_word in tags:\n",
    "        word, tag = tagged_word\n",
    "        if tag=='NN' or tag == 'VBN' or tag == 'NNS' or tag == 'VBP' or tag == 'RB' or tag == 'VBZ' or tag == 'VBG' or tag =='PRP' or tag == 'JJ':\n",
    "            features.append(word)\n",
    "    return features\n",
    "\n",
    "extractedTags = extractTags(tags)\n",
    "print(extractedTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3ae6884b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem', 'stem', 'stem', 'stemmer', 'stem', 'feet', 'will']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "words_for_stemming = ['stem', 'stemming', 'stemmed', 'stemmer', 'stems','feet','willing']\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "[stemmer.stem(x) for x in words_for_stemming]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b1151737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cactus', 'cactus', 'stemming', 'foot', 'foot']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "words = ['cacti', 'cactus', 'stemming', 'feet', 'foot']\n",
    "[lmtzr.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f52eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(text):\n",
    "    words = preprocess(text)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    extracted_features = extractTags(tags)\n",
    "    stemmed_words = [stemmer.stem(x) for x in extracted_features]\n",
    "    result = [lmtzr.lemmatize(x) for x in stemmed_words]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "79021981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', 'oak', 'tree', 'majest', 'center', 'lush', 'green', 'meadow', 'branch', 'reach', 'welcom', 'arm']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'old': True,\n",
       " 'oak': True,\n",
       " 'tree': True,\n",
       " 'majest': True,\n",
       " 'center': True,\n",
       " 'lush': True,\n",
       " 'green': True,\n",
       " 'meadow': True,\n",
       " 'branch': True,\n",
       " 'reach': True,\n",
       " 'welcom': True,\n",
       " 'arm': True}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function to convert features to a dictionary format\n",
    "# Format for NLTK Classifiers >> Efficient Access Purpose\n",
    "# Each feature is represented as a key-value pair, \n",
    "# where the key is the feature name, and the value is either True or False\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "words = extractFeatures(sentence1)\n",
    "print(words)\n",
    "word_feats(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "92883e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_from_doc(data):\n",
    "    result = []\n",
    "    # Corpus - Collection of text\n",
    "    corpus = []\n",
    "    # The responses of the chat bot\n",
    "    answers = {}\n",
    "    for (text,category,answer) in data:\n",
    "\n",
    "        features = extractFeatures(text)\n",
    "\n",
    "        corpus.append(features)\n",
    "        result.append((word_feats(features), category))\n",
    "        answers[category] = answer\n",
    "    combined_corpus = [word for sublist in corpus for word in sublist]\n",
    "    return (result, combined_corpus, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "79b5b612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([({'input': True, 'user': True}, 'category')],\n",
       " ['input', 'user'],\n",
       " {'category': 'answer to give'})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature_from_doc([['this is the input text from the user','category','answer to give'],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eb98a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(filename):\n",
    "    with open(filename, 'r') as content_file:\n",
    "        data = json.load(content_file)\n",
    "        all_data = []\n",
    "        for intent in data['intents']:\n",
    "            for pattern, response in zip(intent['patterns'], intent['responses']):\n",
    "                all_data.append([pattern, intent['tag'], response])\n",
    "    return all_data\n",
    "\n",
    "filename = 'data.json'\n",
    "data = get_content(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a5e07f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data, corpus, answers = extract_feature_from_doc(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99f8ce",
   "metadata": {},
   "source": [
    "## Train model\n",
    "- Classification using Decision tree\n",
    "- Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a074d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "split_ratio = 0.85\n",
    "def split_dataset(data, split_ratio):\n",
    "    random.shuffle(data)\n",
    "    data_length = len(data)\n",
    "    train_split = int(data_length * split_ratio)\n",
    "    return (data[:train_split]), (data[train_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "039a1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = split_dataset(features_data, split_ratio)\n",
    "# save the data\n",
    "np.save('training_data', training_data)\n",
    "np.save('test_data', test_data)\n",
    "# load data\n",
    "training_data = np.load('training_data.npy', allow_pickle=True)\n",
    "test_data = np.load('test_data.npy' , allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "14b728e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_decision_tree(training_data, test_data):\n",
    "    classifier = nltk.classify.DecisionTreeClassifier.train(training_data, entropy_cutoff=0.6, support_cutoff=6)\n",
    "    classifier_name = type(classifier).__name__\n",
    "    training_set_accuracy = nltk.classify.accuracy(classifier, training_data)\n",
    "    print('training set accuracy: ', training_set_accuracy)\n",
    "    test_set_accuracy = nltk.classify.accuracy(classifier, test_data)\n",
    "    print('test set accuracy: ', test_set_accuracy)\n",
    "    return classifier, classifier_name, test_set_accuracy, training_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b028ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set accuracy:  0.8870967741935484\n",
      "test set accuracy:  0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "dtclassifier, classifier_name, test_set_accuracy, training_set_accuracy = train_using_decision_tree(training_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "76957626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_naive_bayes(training_data, test_data):\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_data)\n",
    "    classifier_name = type(classifier).__name__\n",
    "    training_set_accuracy = nltk.classify.accuracy(classifier, training_data)\n",
    "    test_set_accuracy = nltk.classify.accuracy(classifier, test_data)\n",
    "    return classifier, classifier_name, test_set_accuracy, training_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9e6124cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9193548387096774\n",
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "classifier, classifier_name, test_set_accuracy, training_set_accuracy = train_using_naive_bayes(training_data, test_data)\n",
    "print(training_set_accuracy)\n",
    "print(test_set_accuracy)\n",
    "# print(len(classifier.most_informative_features()))\n",
    "# classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "da5ca603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hospitals'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"Hospital\"\n",
    "dtclassifier.classify(word_feats(extractFeatures(input_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e7653111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply1(input_sentence):\n",
    "    category = dtclassifier.classify(word_feats(extractFeatures(input_sentence)))\n",
    "    return answers[category]\n",
    "\n",
    "def reply2(input_sentence):\n",
    "    category = classifier.classify(word_feats(extractFeatures(input_sentence)))\n",
    "    return answers[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c22c6aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1) Lie down or sit down. To reduce the chance of fainting again, don't get up too quickly, 2) Place your head between your knees if you sit down, 3)Position the person on his or her back. If there are no injuries and the person is breathing, raise the person's legs above heart level — about 12 inches (30 centimeters) — if possible. Loosen belts, collars or other constrictive clothing. \""
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'To treat a fever at home: 1)Drink plenty of fluids to stay hydrated. 2)Dress in lightweight clothing. 3)Use a light blanket if you feel chilled, until the chills end. 4)Take acetaminophen (Tylenol, others) or ibuprofen (Advil, Motrin IB, others). 5) Get medical help if the fever lasts more than five days in a row.'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply1('fever treat')\n",
    "reply2('fever treat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7eea8e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can search for pharmacies nearby using online maps or directories.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'You can search for pharmacies nearby using online maps or directories.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply1('pharmacy')\n",
    "reply2('pharmacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "623fbc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To treat a fever at home: 1)Drink plenty of fluids to stay hydrated. 2)Dress in lightweight clothing. 3)Use a light blanket if you feel chilled, until the chills end. 4)Take acetaminophen (Tylenol, others) or ibuprofen (Advil, Motrin IB, others). 5) Get medical help if the fever lasts more than five days in a row.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'To treat a fever at home: 1)Drink plenty of fluids to stay hydrated. 2)Dress in lightweight clothing. 3)Use a light blanket if you feel chilled, until the chills end. 4)Take acetaminophen (Tylenol, others) or ibuprofen (Advil, Motrin IB, others). 5) Get medical help if the fever lasts more than five days in a row.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply1('treat a mild Fever?')\n",
    "reply2('treat a mild Fever?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "61081424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) Wash the affected area with soap and water. Apply a cold compress (such as a flannel or cloth cooled with cold water) or an ice pack to any swelling for at least 10 minutes. Raise or elevate the affected area if possible, as this can help reduce swelling '"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'1) Wash the affected area with soap and water. Apply a cold compress (such as a flannel or cloth cooled with cold water) or an ice pack to any swelling for at least 10 minutes. Raise or elevate the affected area if possible, as this can help reduce swelling '"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply1('how to deal with bleed in nose?')\n",
    "reply2('how to deal with bleed in nose?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
